import feedparser
import google.generativeai as genai
import requests
import time
from datetime import datetime, timedelta
from time import mktime
import os
from bs4 import BeautifulSoup

# Ø¯Ø±ÛŒØ§ÙØª Ú©Ù„ÛŒØ¯Ù‡Ø§
TELEGRAM_TOKEN = os.environ["TELEGRAM_TOKEN"]
CHANNEL_ID = os.environ["CHANNEL_ID"]
GEMINI_API_KEY = os.environ["GEMINI_API_KEY"]

# Ù…Ù†Ø¨Ø¹: Ø²ÙˆÙ…ÛŒØª
RSS_URLS = [
    "https://www.zoomit.ir/feed/",
]

# Ù…Ø¯Ù„ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ø¨Ø§ Ø³Ù‡Ù…ÛŒÙ‡ 1000 ØªØ§ÛŒÛŒ
genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-2.5-flash-lite')

HISTORY_FILE = "history.txt"

def load_history():
    if not os.path.exists(HISTORY_FILE): return []
    with open(HISTORY_FILE, "r", encoding="utf-8") as f:
        return [line.strip() for line in f.readlines() if line.strip()]

def save_to_history(link, title):
    try:
        with open(HISTORY_FILE, "a", encoding="utf-8") as f:
            f.write(f"{link}|{title}\n")
        os.system(f'git config --global user.name "News Bot"')
        os.system(f'git config --global user.email "bot@noreply.github.com"')
        os.system(f'git add {HISTORY_FILE}')
        os.system('git commit -m "Update history"')
        os.system('git push')
    except: pass

def check_is_duplicate_topic(new_title, history_lines):
    recent_titles = [line.split("|")[1] for line in history_lines[-50:] if len(line.split("|")) > 1]
    if not recent_titles: return False
    
    prompt = f"""
    Ù„ÛŒØ³Øª ØªÛŒØªØ±Ù‡Ø§ÛŒ Ø§Ø®ÛŒØ±: {recent_titles}
    ØªÛŒØªØ± Ø¬Ø¯ÛŒØ¯: '{new_title}'
    Ø¢ÛŒØ§ Ø§ÛŒÙ† ØªÛŒØªØ± Ø¬Ø¯ÛŒØ¯ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ù‡Ù…Ø§Ù† Ø®Ø¨Ø±ÛŒ Ø§Ø³Øª Ú©Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ ÙØ±Ø³ØªØ§Ø¯ÛŒÙ…ØŸ (Ø­ØªÛŒ Ø§Ú¯Ø± Ú©Ù„Ù…Ø§ØªØ´ Ú©Ù…ÛŒ ÙØ±Ù‚ Ø¯Ø§Ø±Ø¯).
    ÙÙ‚Ø· Ø¨Ù†ÙˆÛŒØ³: YES ÛŒØ§ NO
    """
    try:
        res = model.generate_content(prompt).text.strip().upper()
        time.sleep(2)
        return "YES" in res
    except: 
        return False

def send_to_telegram(message, image_url=None):
    try:
        url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/" + ("sendPhoto" if image_url else "sendMessage")
        data = {"chat_id": CHANNEL_ID, "parse_mode": "Markdown"}
        if image_url:
            data["photo"] = image_url
            data["caption"] = message
        else:
            data["text"] = message
        requests.post(url, data=data)
    except Exception as e: print(f"Send Error: {e}")

def extract_image(entry):
    try:
        if 'media_content' in entry: return entry.media_content[0]['url']
        if 'links' in entry:
            for l in entry.links:
                if l.type.startswith('image/'): return l.href
        if 'summary' in entry:
            soup = BeautifulSoup(entry.summary, 'html.parser')
            img = soup.find('img')
            if img: return img['src']
    except: pass
    return None

def summarize_with_ai(title, content):
    # Ø¯Ø³ØªÙˆØ± Ø¢Ù¾Ø¯ÛŒØª Ø´Ø¯Ù‡ Ø¨Ø§ Ø·ÙˆÙ„ Ù…ØªÙ† Ø¨ÛŒØ´ØªØ± (5 ØªØ§ 11 Ø®Ø·)
    prompt = f"""
    ØªÙˆ Ø³Ø±Ø¯Ø¨ÛŒØ± Ø§Ø±Ø´Ø¯ ÛŒÚ© Ú©Ø§Ù†Ø§Ù„ ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ ÙØ§Ø±Ø³ÛŒ Ù‡Ø³ØªÛŒ.
    ØªÛŒØªØ± Ø®Ø¨Ø±: {title}
    Ù…ØªÙ† Ø®Ø¨Ø±: {content}

    ÙˆØ¸Ø§ÛŒÙ ØªÙˆ:
    1. **Ø®Ù„Ø§ØµÙ‡ Ø³Ø§Ø²ÛŒ Ø¯Ù‚ÛŒÙ‚:** Ù…ØªÙ† Ø±Ø§ ØªØ­Ù„ÛŒÙ„ Ú©Ù† Ùˆ Ù†Ú©Ø§Øª Ø¬Ø°Ø§Ø¨ØŒ Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ Ø§Ø±Ù‚Ø§Ù… Ù…Ù‡Ù… Ùˆ Ø¬Ø²Ø¦ÛŒØ§Øª ÙÙ†ÛŒ Ø±Ø§ Ø¨ÛŒØ±ÙˆÙ† Ø¨Ú©Ø´.
    2. **Ø·ÙˆÙ„ Ù…ØªÙ†:** Ø¯Ø³ØªØª Ø¨Ø§Ø² Ø§Ø³Øª. Ù…ØªÙ† Ø¨Ø§ÛŒØ¯ "Ú©Ø§Ù…Ù„ Ùˆ Ù¾Ø±Ù…Ø­ØªÙˆØ§" Ø¨Ø§Ø´Ø¯ (Ø¨ÛŒÙ† 5 ØªØ§ 11 Ø®Ø·). Ø§Ú¯Ø± Ø®Ø¨Ø± Ù…Ù‡Ù…ÛŒ Ø§Ø³ØªØŒ ØªØ§ 11 Ø®Ø· Ø¨Ù†ÙˆÛŒØ³ ØªØ§ Ø­Ù‚ Ù…Ø·Ù„Ø¨ Ø§Ø¯Ø§ Ø´ÙˆØ¯. Ø§Ú¯Ø± Ø®Ø¨Ø± Ú©ÙˆØªØ§Ù‡ Ø§Ø³ØªØŒ Ù‡Ù…Ø§Ù† 5 Ø®Ø· Ú©Ø§ÙÛŒØ³Øª.
    3. **Ù„Ø­Ù†:** Ø­Ø±ÙÙ‡â€ŒØ§ÛŒØŒ Ø±ÙˆØ§Ù† Ùˆ Ú˜ÙˆØ±Ù†Ø§Ù„ÛŒØ³ØªÛŒ (Ù†Ù‡ Ø®Ø´Ú©ØŒ Ù†Ù‡ Ù„ÙˆØ¯Ú¯ÛŒ). Ø§Ø² Ø§ÛŒÙ…ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†.
    4. **Ù¾Ø§ÙˆØ±Ù‚ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ (Ø¨Ø³ÛŒØ§Ø± Ù…Ù‡Ù…):**
       - Ø§Ú¯Ø± Ø¯Ø± Ù…ØªÙ† Ø§Ø³Ù… Ø´Ø±Ú©ØªØŒ Ø§Ø³ØªØ§Ø±ØªØ§Ù¾ØŒ ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ Ø®Ø§Øµ ÛŒØ§ ÙØ±Ø¯ÛŒ Ø¢Ù…Ø¯Ù‡ Ú©Ù‡ "Ú¯Ù…Ù†Ø§Ù…" Ø§Ø³Øª (Û¹Û°Ùª Ù…Ø®Ø§Ø·Ø¨Ø§Ù† Ø¹Ø§Ù… Ù†Ù…ÛŒâ€ŒØ´Ù†Ø§Ø³Ù†Ø¯) Ùˆ Ø¯Ø± Ù…ØªÙ† ØªÙˆØ¶ÛŒØ­ÛŒ Ù†Ø¯Ø§Ø±Ø¯:
       - Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ÛŒ Ù¾ÛŒØ§Ù… ÛŒÚ© Ø®Ø· Ø¨Ø§ Ø¹Ù„Ø§Ù…Øª ðŸ’¡ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù† Ùˆ Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡ Ùˆ Ù…ÙÛŒØ¯ (Ù†ØµÙ Ø®Ø·) Ø¢Ù† Ø±Ø§ Ù…Ø¹Ø±ÙÛŒ Ú©Ù†.
       - Ù…Ø«Ø§Ù„: "ðŸ’¡ Ø¢Ù†ØªØ±ÙˆÙ¾ÛŒÚ©: Ø§Ø³ØªØ§Ø±ØªØ§Ù¾ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø±Ù‚ÛŒØ¨ OpenAI."
       - Ø§Ú¯Ø± Ù‡Ù…Ù‡ Ú†ÛŒØ² Ù…Ø¹Ø±ÙˆÙ Ø¨ÙˆØ¯ (Ù…Ø«Ù„ Ø§Ù¾Ù„ØŒ Ø³Ø§Ù…Ø³ÙˆÙ†Ú¯ØŒ Ø§ÛŒÙ„Ø§Ù† Ù…Ø§Ø³Ú©)ØŒ Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø±Ø§ Ú©Ù„Ø§Ù‹ Ù†Ù†ÙˆÛŒØ³.
    
    5. **Ù‚Ø§Ù„Ø¨ Ø¨Ù†Ø¯ÛŒ:**
       - Ù‡ÛŒÚ† Ù„ÛŒÙ†Ú©ÛŒ Ø¯Ø± Ù…ØªÙ† Ù†Ú¯Ø°Ø§Ø±.
       - Ø¯Ø± Ø®Ø· Ø¢Ø®Ø± ÙÙ‚Ø· Ø¨Ù†ÙˆÛŒØ³: ðŸ†” @Teklp
    """
    try: 
        response = model.generate_content(prompt).text
        time.sleep(4) 
        return response
    except: return None

def check_feeds():
    history_lines = load_history()
    history_links = [line.split("|")[0] for line in history_lines]
    
    # ØªØ§ÛŒÙ… 150 Ø¯Ù‚ÛŒÙ‚Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ú©Ø§Ù…Ù„
    time_threshold = datetime.now() - timedelta(minutes=150)
    
    for url in RSS_URLS:
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries:
                if hasattr(entry, 'published_parsed'):
                    pub_date = datetime.fromtimestamp(mktime(entry.published_parsed))
                    
                    if pub_date > time_threshold:
                        if entry.link in history_links: continue
                        
                        if check_is_duplicate_topic(entry.title, history_lines):
                            save_to_history(entry.link, entry.title)
                            continue
                        
                        summary = summarize_with_ai(entry.title, entry.summary)
                        
                        if summary:
                            final_text = f"ðŸ”¥ **{entry.title}**\n\n{summary}"
                            send_to_telegram(final_text, extract_image(entry))
                            save_to_history(entry.link, entry.title)
                            time.sleep(5)
        except Exception as e: print(f"Feed Error: {e}")

if __name__ == "__main__":
    check_feeds()
